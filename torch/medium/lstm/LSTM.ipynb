{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cblh/TorchLeet/blob/main/torch/medium/lstm/LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZCFIJZl8T_z"
      },
      "source": [
        "# Problem: Implement an LSTM Model\n",
        "\n",
        "### Problem Statement\n",
        "You are tasked with implementing a simple **LSTM (Long Short-Term Memory)** model in PyTorch. The model should process sequential data using an LSTM layer followed by a fully connected (FC) layer. Your goal is two-fold: one is to implement a LSTM layer from scratch and another using inbuilt pytorch LSTM layer. Compare the results implementing the forward passes for both the LSTM models.\n",
        "\n",
        "### Requirements\n",
        "1. **Define the LSTM Model using Custom LSTM layer**:\n",
        "   - Add a `Custom` LSTM layer to the model. The layer must take care of the hidden and cell states\n",
        "   - Add a **fully connected (FC) layer** that maps the output of the LSTM to the final predictions.\n",
        "   - Implement the `forward` method to:\n",
        "     - Pass the input sequence through the LSTM.\n",
        "     - Feed the output of the LSTM into the fully connected layer for the final output.\n",
        "\n",
        "2. **Define the LSTM Model using in-built LSTM layer**:\n",
        "  - Same as `1` with only difference that this time define the LSTM layer using pytorch `nn.Module`\n",
        "\n",
        "### Constraints\n",
        "- The LSTM layer should be implemented with a single hidden layer.\n",
        "- Use a suitable number of input features, hidden units, and output size for the task.\n",
        "- Make sure the `forward` method returns the output of the fully connected layer after processing the LSTM output.\n",
        "\n",
        "\n",
        "<details>\n",
        "  <summary>üí° Hint</summary>\n",
        "  Add the LSTM layer and FC layer in LSTMModel.__init__.\n",
        "  <br>\n",
        "  Implement the forward pass to process sequences using the LSTM and FC layers.\n",
        "  <br> Review Hidden and cell states computation here: [D2l.ai](https://d2l.ai/chapter_recurrent-modern/lstm.html)\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Lc7TlirO8T_1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "11KhMbFr8T_3"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic sequential data\n",
        "torch.manual_seed(42)\n",
        "sequence_length = 10\n",
        "num_samples = 100\n",
        "\n",
        "# Create a sine wave dataset\n",
        "X = torch.linspace(0, 4 * 3.14159, steps=num_samples).unsqueeze(1)\n",
        "y = torch.sin(X)\n",
        "\n",
        "# Prepare data for LSTM\n",
        "def create_in_out_sequences(data, seq_length):\n",
        "    in_seq = []\n",
        "    out_seq = []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        in_seq.append(data[i:i + seq_length])\n",
        "        out_seq.append(data[i + seq_length])\n",
        "    return torch.stack(in_seq), torch.stack(out_seq)\n",
        "\n",
        "X_seq, y_seq = create_in_out_sequences(y, sequence_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "L3aVOUTV8T_3"
      },
      "outputs": [],
      "source": [
        "class CustomLSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_units):\n",
        "        def normal(shape):\n",
        "            return torch.randn(size=shape)*0.01\n",
        "        def three():\n",
        "            return (normal((input_dim, hidden_units)),\n",
        "                   normal((hidden_units, hidden_units)),\n",
        "                   torch.zeros(hidden_units))\n",
        "\n",
        "        self.W_xi, self.W_hi, self.b_i = three()  # ËæìÂÖ•Èó®ÂèÇÊï∞\n",
        "        self.W_xf, self.W_hf, self.b_f = three()  # ÈÅóÂøòÈó®ÂèÇÊï∞\n",
        "        self.W_xo, self.W_ho, self.b_o = three()  # ËæìÂá∫Èó®ÂèÇÊï∞\n",
        "        self.W_xc, self.W_hc, self.b_c = three()  # ÂÄôÈÄâËÆ∞ÂøÜÂÖÉÂèÇÊï∞\n",
        "        # ËæìÂá∫Â±ÇÂèÇÊï∞\n",
        "        self.fc = nn.Linear(hidden_units, 1)\n",
        "        self.hidden_units = hidden_units\n",
        "\n",
        "    def forward(self, inputs, H_C=None):\n",
        "        def init_lstm_state(batch_size, num_hiddens, device):\n",
        "            return (torch.zeros((batch_size, num_hiddens)),\n",
        "                    torch.zeros((batch_size, num_hiddens)))\n",
        "        batch_size, seq_lens, _ = inputs.size()\n",
        "        if H_C is None:\n",
        "            H_C = init_lstm_state(batch_size, self.hidden_units)\n",
        "        [W_xi, W_hi, b_i, W_xf, W_hf, b_f, W_xo, W_ho, b_o, W_xc, W_hc, b_c,\n",
        "        W_hq, b_q] = self\n",
        "        (H, C) = H_C\n",
        "        outputs = []\n",
        "        for X in inputs:\n",
        "            I = torch.sigmoid((X @ W_xi) + (H @ W_hi) + b_i)\n",
        "            F = torch.sigmoid((X @ W_xf) + (H @ W_hf) + b_f)\n",
        "            O = torch.sigmoid((X @ W_xo) + (H @ W_ho) + b_o)\n",
        "            C_tilda = torch.tanh((X @ W_xc) + (H @ W_hc) + b_c)\n",
        "            C = F * C + I * C_tilda\n",
        "            H = O * torch.tanh(C)\n",
        "            Y = (H @ W_hq) + b_q\n",
        "            outputs.append(Y)\n",
        "        return torch.cat(outputs, dim=0), (H, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oxON1lWw8T_3"
      },
      "outputs": [],
      "source": [
        "# Define the LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm = nn.LSTM(input_size=1, hidden_size=50, num_layers=1, batch_first=True)\n",
        "        self.fc = nn.Linear(50, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])  # Use the last output of the LSTM\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tUuZWh_D8T_4",
        "outputId": "24c5071f-81b2-4f81-9ea2-d8560aaae887",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "cannot assign module before Module.__init__() call",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1068928714.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize the model, loss function, and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCustomLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mmodel_inbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer_custom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_custom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2178976203.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dim, hidden_units)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_xc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_hc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# ÂÄôÈÄâËÆ∞ÂøÜÂÖÉÂèÇÊï∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# ËæìÂá∫Â±ÇÂèÇÊï∞\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_units\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_units\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_units\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   1999\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmodules\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m                     raise AttributeError(\n\u001b[0m\u001b[1;32m   2002\u001b[0m                         \u001b[0;34m\"cannot assign module before Module.__init__() call\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m                     )\n",
            "\u001b[0;31mAttributeError\u001b[0m: cannot assign module before Module.__init__() call"
          ]
        }
      ],
      "source": [
        "# Initialize the model, loss function, and optimizer\n",
        "model_custom = CustomLSTMModel(1, 50)\n",
        "model_inbuilt = LSTMModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer_custom = optim.Adam(model_custom.parameters(), lr=0.01)\n",
        "optimizer_inbuilt = optim.Adam(model_inbuilt.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zMxblwp8T_4",
        "outputId": "0acdc48e-a935-43b7-d6e6-6ef530bfad1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [50/500], Loss: 0.0004\n",
            "Epoch [100/500], Loss: 0.0000\n",
            "Epoch [150/500], Loss: 0.0000\n",
            "Epoch [200/500], Loss: 0.0000\n",
            "Epoch [250/500], Loss: 0.0000\n",
            "Epoch [300/500], Loss: 0.0000\n",
            "Epoch [350/500], Loss: 0.0000\n",
            "Epoch [400/500], Loss: 0.0000\n",
            "Epoch [450/500], Loss: 0.0000\n",
            "Epoch [500/500], Loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Training loop for the custom model\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    state = None\n",
        "    pred, state = model_custom(X_seq, state)\n",
        "    loss = criterion(pred[:, -1, :], y_seq) # Use the last output of the LSTM\n",
        "    # Backward pass and optimization\n",
        "    optimizer_custom.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_custom.step()\n",
        "\n",
        "    # Log progress every 50 epochs\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bobay0Ms8T_5"
      },
      "outputs": [],
      "source": [
        "# Training loop for the inbuilt model\n",
        "epochs = 500\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    pred = model_inbuilt(X_seq)\n",
        "    loss = criterion(pred, y_seq)\n",
        "    # Backward pass and optimization\n",
        "    optimizer_inbuilt.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer_inbuilt.step()\n",
        "\n",
        "    # Log progress every 50 epochs\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVfZVN6T8T_5",
        "outputId": "f5ddae26-d98f-4b18-be8f-5daf6e6859bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predictions for new sequence: [1.0354082584381104, 1.0123006105422974, 0.9615825414657593, 0.8840561509132385, 0.7813034653663635, 0.6558271050453186, 0.5111342668533325, 0.3516756296157837, 0.18258695304393768, 0.009290406480431557]\n"
          ]
        }
      ],
      "source": [
        "# Testing on new data\n",
        "test_steps = 100  # Ensure this is greater than sequence_length\n",
        "X_test = torch.linspace(0, 5 * 3.14159, steps=test_steps).unsqueeze(1)\n",
        "y_test = torch.sin(X_test)\n",
        "\n",
        "# Create test input sequences\n",
        "X_test_seq, _ = create_in_out_sequences(y_test, sequence_length)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred_custom, _ = model_custom(X_test_seq)\n",
        "    pred_inbuilt = model_inbuilt(X_test_seq)\n",
        "pred_custom = torch.flatten(pred_custom[:, -1, :])\n",
        "pred_inbuilt = pred_inbuilt.squeeze()\n",
        "print(f\"Predictions with Custom Model for new sequence: {pred_custom.tolist()}\")\n",
        "print(f\"Predictions with In-Built Model: {pred_inbuilt.tolist()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzbVBmH38T_5"
      },
      "outputs": [],
      "source": [
        "#Plot the predictions\n",
        "plt.figure()\n",
        "# plt.plot(y_test, label=\"Ground Truth\")\n",
        "plt.plot(pred_custom, label=\"custom model\")\n",
        "plt.plot(pred_inbuilt, label=\"inbuilt model\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}